from enum import Enum
from numbers import Number
from typing import Tuple, Dict, List, Any

import pandas
import numpy as np
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.feature_selection import mutual_info_regression
from sklearn.utils.validation import check_X_y

import spectraxai.utils.kennardStone as kennardStone
from spectraxai.spectra import Spectra, SpectralPreprocessingSequence


class Scale(str, Enum):
    """Scaling of an input feature (or of the output) supported by the `Dataset` class"""

    STANDARD = "standard"
    """Standard scaling, i.e. removing the mean and scaling to unit variance"""
    MINMAX = "min-max"
    """Scale and translate so that the range is between zero and one"""

    def __str__(self):
        return self.name

    def __repr__(self):
        return self.value


class DatasetSplit(str, Enum):
    """Types of dataset split supported by the `Dataset` class"""

    RANDOM = "random"
    KENNARD_STONE = "Kennard-Stone"
    CLHS = "clhs"
    CROSS_VALIDATION = "cross-validation"
    STRATIFIED = "stratified"

    def __str__(self):
        return self.name

    def __repr__(self):
        return self.value


DataSplit = Tuple[
    np.ndarray,  # idx_trn
    np.ndarray,  # idx_tst
]
"""A tuple representing a split for the `Dataset` into training and testing indices"""


class Dataset:
    """
    A general class to manage the dataset (i.e. input X and output Y).

    Use this class to pass your 2D spectral matrix and 1D or 2D output properties.
    Supports methods for pre-processing X, scaling X and Y, splitting the dataset, and more.
    """

    n_samples: int
    """Number of samples in the dataset"""
    n_features: int
    """Number of features in the dataset's input"""
    n_outputs: int
    """Number of outputs in the dataset's output"""
    X: np.ndarray
    """The spectral data of size (`n_samples`, `n_features`)"""
    Y: np.ndarray
    """The output properties of size (`n_samples`, `n_outputs`)"""
    X_names: List[str]
    """The names of the input features of size (`n_features`)"""
    Y_names: List[str]
    """The names of the output properties of size (`n_outputs`)"""

    def __init__(
        self,
        X: np.ndarray,
        Y: np.ndarray,
        X_names: List[str] = [],
        Y_names: List[str] = [],
    ):
        """
        Create a new Dataset from input and output arrays

        Parameters
        ----------

        X: `numpy.ndarray`
            A 2D matrix of the spectra of size (`n_samples`, `n_features`)

        Y: `numpy.ndarray`
            A 1D vector (`n_samples`,) or 2D matrix (`n_samples`, `n_outputs`) of the output property(ies).
            If 1D it will be implicitly converted to 2D.

        X_names: `list[str]`, optional
            A list of length n_features containing the names of the input features.
            If missing, these will be autogenerated as X1, X2, etc.

        Y_names: `list[str]`, optional
            A list of length n_outputs containing the names of the output properties.
            When using a single output property pass a list of size 1.
            If missing, these will be autogenerated as Y1, Y2, etc.
        """
        if X.shape[0] != Y.shape[0]:
            raise AssertionError("X and Y don't have the same number of rows!")
        if X.ndim != 2:
            raise AssertionError("X should have exactly two dimensions")
        self.X = X.to_numpy() if isinstance(X, pandas.DataFrame) else X
        if isinstance(Y, pandas.DataFrame) or isinstance(Y, pandas.Series):
            Y = Y.to_numpy()
        self.Y = Y if Y.ndim > 1 else Y.reshape(-1, 1)
        check_X_y(X, Y, multi_output=True)
        self.n_samples = self.X.shape[0]
        self.n_features = self.X.shape[1]
        self.n_outputs = self.Y.shape[1]
        if X_names and len(X_names) != self.n_features:
            raise AssertionError("X_names should have the same n_features as X")
        if Y_names and len(Y_names) != self.n_outputs:
            raise AssertionError("Y_names should have the same n_outputs as Y")
        self.X_names = (
            X_names if X_names else ["X" + str(i + 1) for i in range(self.n_features)]
        )
        self.Y_names = (
            Y_names if Y_names else ["Y" + str(i + 1) for i in range(self.n_outputs)]
        )

    def train_test_split(self, split: DatasetSplit, opt: Number) -> DataSplit:
        """
        Splits dataset with passed split method to train and test.

        Parameters
        ----------

        split: `DatasetSplit`
                The method used to split the dataset

        opt: `Number`
                A float number (between 0 and 1) indicating the percentage of the training dataset for Random and Kennardâ€“Stone split.
                A natural number for Cross Validation and Stratified split.

        Returns
        -------
        `DataSplit`
            The idx_trn, idx_tst tuple. Use e.g. X[idx_trn], Y[idx_trn] to get the training dataset.
        """
        indices = np.arange(self.X.shape[0])
        if split == DatasetSplit.RANDOM or split == DatasetSplit.KENNARD_STONE:
            msg = "opt param should be a float in the (0, 1) range for RANDOM and KENNARD_STONE splits"
            if not isinstance(opt, float):
                raise TypeError(msg)
            if opt <= 0 or opt >= 1:
                raise ValueError(msg)
        elif split == DatasetSplit.CROSS_VALIDATION or split == DatasetSplit.STRATIFIED:
            msg = "opt param should be a positive int greater than 1 for CROSS_VALIDATION and STRATIFIED splits"
            if not isinstance(opt, int):
                raise TypeError(msg)
            if opt <= 1:
                raise ValueError(msg)
        if split == DatasetSplit.RANDOM:
            return train_test_split(indices, train_size=opt)
        elif split == DatasetSplit.KENNARD_STONE:
            _, _, idx_trn, idx_tst = kennardStone.train_test_split(
                self.X, indices, test_size=(1 - opt)
            )
            return idx_trn, idx_tst
        elif split == DatasetSplit.CLHS:
            raise NotImplementedError("clhs not implemented yet")
        elif split == DatasetSplit.CROSS_VALIDATION:
            kf = KFold(opt, shuffle=True)
            idx_trn, idx_tst = [], []
            for trn_index, tst_index in kf.split(self.X):
                idx_trn.append(np.array(trn_index))
                idx_tst.append(np.array(tst_index))
            return np.array(idx_trn, dtype=object), np.array(idx_tst, dtype=object)
        elif split == DatasetSplit.STRATIFIED:
            skf = StratifiedKFold(n_splits=opt)
            idx_trn, idx_tst = [], []
            for trn_index, tst_index in skf.split(self.X, self.Y):
                idx_trn.append(np.array(trn_index))
                idx_tst.append(np.array(tst_index))
            return np.array(idx_trn, dtype=object), np.array(idx_tst, dtype=object)
        else:
            raise RuntimeError("Not a valid split method!")

    def subset(self, idx: np.ndarray) -> "Dataset":
        """
        Subset the dataset using passed indices

        Parameters
        ----------
        idx: np.ndarray
            The indices to subset by

        Returns
        -------
        `Dataset`
            A new subsetted Dataset

        """
        if idx.size == 0:
            raise ValueError("Passed indices for subsetting cannot be empty")
        if idx.size > 0 and isinstance(idx[0], np.ndarray):
            raise ValueError("Expected indices array to contain only one dimension")
        return Dataset(self.X[idx], self.Y[idx], self.X_names, self.Y_names)

    def train_test_split_explicit(
        self, trn: np.array = np.array([]), tst: np.array = np.array([])
    ) -> DataSplit:
        """
        Splits dataset to train and test from pre-selected by the user trn or tst indices.

        Parameters
        ----------
        trn: np.ndarray, optional
            Contains the indices of the training samples

        tst: np.ndarray, optional
            Contains the indices of the testing samples

        Returns
        -------
        `DataSplit`
            The idx_trn, idx_tst tuple
        """
        if tst.size == 0 and trn.size == 0:
            raise AssertionError("You need to specify either tst or trn indices")
        if tst.size > 0 and trn.size > 0:
            raise AssertionError("You cannot specify both trn and tst")
        if tst.size > 0 and isinstance(tst[0], np.ndarray):
            trn = np.array(
                [self.train_test_split_explicit(tst=fold)[0] for fold in tst]
            )
        elif tst.size > 0:
            if not np.logical_and(tst >= 0, tst <= self.n_samples).all():
                raise AssertionError("Passed indices contain out of bound values")
            trn = np.array(list(set(range(0, self.n_samples)).difference(set(tst))))
        elif trn.size > 0 and isinstance(trn[0], np.ndarray):
            tst = np.array(
                [self.train_test_split_explicit(trn=fold)[1] for fold in trn]
            )
        else:
            if not np.logical_and(trn >= 0, trn <= self.n_samples).all():
                raise AssertionError("Passed indices contain out of bound values")
            tst = np.array(list(set(range(0, self.n_samples)).difference(set(trn))))
        return trn, tst

    def preprocess(self, method: SpectralPreprocessingSequence) -> "Dataset":
        """
        Preprocess dataset by method.

        Parameters
        ----------

        method: `spectraxai.spectra.SpectralPreprocessingSequence`
            The method for the preprocess.

        Returns
        -------
        `Dataset`
            A new Dataset object.
        """
        return Dataset(
            self.__preprocess(self.X, method), self.Y, self.X_names, self.Y_names
        )

    def preprocess_3D(self, methods: List[SpectralPreprocessingSequence]):
        """
        Preprocess spectra into a new 3D matrix by methods in a list structure.

        Parameters
        ----------

        methods: `List[SpectralPreprocessingSequence]`
            The methods for the preprocess.

        Returns
        -------
        `numpy.ndarray`
            A 3D matrix of size (`n_samples`, `n_features`, n_preprocesses)
        """
        if len(methods) <= 1:
            raise AssertionError(
                "A 3D matrix must contain at least two pre-processing sequences"
            )
        X = np.empty((self.X.shape[0], self.X.shape[1], len(methods)))
        for i, method in enumerate(methods):
            thisX = np.copy(self.X)
            X[:, :, i] = self.__preprocess(thisX, method)
        return X

    def corr(self) -> np.ndarray:
        """
        Calculate Pearson's correlation between all input features and the outputs

        Returns
        ------
        `np.ndarray`
            A 2-D np.array containing the correlation for each output property of size (`n_outputs`, `n_features`)
        """
        return np.array(
            [
                [
                    np.corrcoef(self.X[:, i], self.Y[:, j])[0][1]
                    for i in range(self.n_features)
                ]
                for j in range(self.n_outputs)
            ]
        )

    def mi(self) -> np.ndarray:
        """
        Calculate the mutual information between all input features and the outputs

        Returns
        ------
        `np.ndarray`
            A 2-D np.array containing the mutual information for each output property of size (`n_outputs`, `n_features`)
        """
        normalize = lambda a: a / np.max(a)
        return np.array(
            [
                normalize(mutual_info_regression(self.X, self.Y[:, j]))
                for j in range(self.n_outputs)
            ]
        )

    def apply_unscale_X(
        self,
        method: Scale,
        set_params: List = [],
        set_attributes: List = [],
        X: np.ndarray = np.array([]),
    ):
        """
        Unscale X matrix of the spectra with Scale method.

        Parameters
        ----------

        method: `Scale`
                The method is used to scale X

        set_params: `List`
                A list of dicts with the parameters of each Scale method.

        set_attributes: `List`
                A list of dicts with the attributes of each Scale method.

        X: `numpy.ndarray`
                A 2D or 3D matrix of the spectra for scaled X hat

        Returns
        -------
        `numpy.ndarray`
            The original X matrix of the spectra. If X, set_params and set_attributes have been given for parameters.
        or
        `Dataset`
            A Dataset object.
        """
        if X.size > 0 and len(self.get_scale_X_props) > 0:
            return Dataset.unscale_X(
                X,
                method,
                self.get_scale_X_props["params"],
                self.get_scale_X_props["attributes"],
            )
        elif len(self.get_scale_X_props) > 0:
            self.X = Dataset.unscale_X(
                self.X,
                method,
                self.get_scale_X_props["params"],
                self.get_scale_X_props["attributes"],
            )
        else:
            self.X = Dataset.unscale_X(self.X, method, set_params, set_attributes)
        return self

    def unscale_X(
        X: np.ndarray, method: Scale, set_params: List = [], set_attributes: List = []
    ):
        """
        Static unscale for X matrix of the spectra with Scale method.

        Parameters
        ----------

        X: `numpy.ndarray`
                A 2D or 3D scaled matrix of the spectra.

        method: `Scale`
                The method is used to scale X

        set_params: `List`
                A list of dicts with the parameters of each Scale method.

        set_attributes: `List`
                A list of dicts with the attributes of each Scale method.


        Returns
        -------
        `numpy.ndarray`
            The original X matrix of the spectra.
        """
        if len(set_attributes) == 0:
            raise AssertionError("You need to specify set_attributes")
        if X.ndim not in [2, 3]:
            raise AssertionError("X must be either a 2-D or a 3-D matrix")
        if X.ndim == 3:
            if method == Scale.STANDARD:
                scaler = [
                    [StandardScaler() for _ in range(X.shape[1])]
                    for _ in range(X.shape[2])
                ]
            elif method == Scale.MINMAX:
                scaler = [
                    [MinMaxScaler() for _ in range(X.shape[1])]
                    for _ in range(X.shape[2])
                ]
            for i in range(X.shape[2]):
                X[:, :, i] = Dataset.__unscale_X_parser(
                    X[:, :, i], method, scaler[i], set_params[i], set_attributes[i]
                )
        else:
            if method == Scale.STANDARD:
                scaler = [StandardScaler() for _ in range(X.shape[1])]
            elif method == Scale.MINMAX:
                scaler = [MinMaxScaler() for _ in range(X.shape[1])]
            X = Dataset.__unscale_X_parser(
                X, method, scaler, set_params[0], set_attributes[0]
            )
        return X

    def __unscale_X_parser(
        X: np.ndarray,
        method: Scale,
        scaler: Any,
        set_params: Dict,
        set_attributes: Dict,
    ):
        for i in range(X.shape[1]):
            if len(set_params) != 0:
                scaler[i] = scaler[i].set_params(**set_params[i])
            scaler[i] = Dataset.__set_scale_attributes(
                method, scaler[i], set_attributes[i]
            )
            X[:, i] = scaler[i].inverse_transform(X[:, i].reshape(-1, 1)).flatten()
        return X

    def apply_scale_X(
        self, method: Scale, set_params: List = [], set_attributes: List = []
    ):
        """
        Scale X matrix of the spectra with Scale method.

        Parameters
        ----------

        method: `Scale`
                The method used is to scale 2D or 3D X matrix of the spectra.

        set_params: `List`
                A list of dicts with the parameters of each Scale method.

        set_attributes: `List`
                A list of dicts with the attributes of each Scale method.


        Returns
        -------
        `Dataset`
             A Dataset object.
        """
        self.X, self.get_scale_X_props = Dataset.scale_X(
            self.X, method, set_params, set_attributes
        )
        return self

    def scale_X(
        X: np.ndarray, method: Scale, set_params: List = [], set_attributes: List = []
    ):
        """
        Static scale method of X matrix of the spectra with Scale method.

        Parameters
        ----------

        X: `numpy.ndarray`
                A 2D or 3D matrix of the spectra.

        method: `Scale`
                The method is used to scale X

        set_params: `List`
                A list of dicts with the parameters of each Scale method.

        set_attributes: `List`
                A list of dicts with the attributes of each Scale method.


        Returns
        -------
        `Dataset`
             A Dataset object.
        """
        if X.ndim == 3:
            if method == Scale.STANDARD:
                scaler = [
                    [StandardScaler() for _ in range(X.shape[1])]
                    for _ in range(X.shape[2])
                ]
            elif method == Scale.MINMAX:
                scaler = [
                    [MinMaxScaler() for _ in range(X.shape[1])]
                    for _ in range(X.shape[2])
                ]
            get_params = []
            get_attributes = []
            for i in range(X.shape[2]):
                X[:, :, i], params, attributes = Dataset.__scale_X_parser(
                    X[:, :, i],
                    method,
                    scaler[i],
                    set_params[i] if len(set_params) else [],
                    set_attributes[i] if len(set_attributes) else [],
                )
                get_params.append(params)
                get_attributes.append(attributes)
        else:
            if method == Scale.STANDARD:
                scaler = [StandardScaler() for _ in range(X.shape[1])]
            elif method == Scale.MINMAX:
                scaler = [MinMaxScaler() for _ in range(X.shape[1])]
            X, params, attributes = Dataset.__scale_X_parser(
                X,
                method,
                scaler,
                set_params[0] if len(set_params) else [],
                set_attributes[0] if len(set_attributes) else [],
            )
            get_params = [params]
            get_attributes = [attributes]
        return X, {"params": get_params, "attributes": get_attributes}

    def __scale_X_parser(
        X: np.ndarray,
        method: Scale,
        scaler: Any,
        set_params: Dict,
        set_attributes: Dict,
    ):
        get_params = []
        get_attributes = []
        for i in range(X.shape[1]):
            temp_X, params, attributes = Dataset.__scale_X(
                X[:, i].reshape(-1, 1),
                method,
                scaler[i],
                set_params[i] if len(set_params) else {},
                set_attributes[i] if len(set_attributes) else {},
            )
            X[:, i] = temp_X.flatten()
            get_params.append(params)
            get_attributes.append(attributes)
        return X, get_params, get_attributes

    def apply_unscale_Y(
        self,
        method: Scale,
        set_params: List = [],
        set_attributes: List = [],
        Y: np.ndarray = np.array([]),
    ):
        """
        Unscale a 1D vector or 2D matrix of the output property(ies) with Scale method.

        Parameters
        ----------

        method: `Scale`
                The method is used to scale Y

        set_params: `List`
                A list of dicts with the parameters of each Scale method.

        set_attributes: `List`
                A list of dicts with the attributes of each Scale method.

        Y: `numpy.ndarray`
                A 1D vector or 2D matrix of the output property(ies).

        Returns
        -------
        `numpy.ndarray`
            The original 1D or 2D matrix Y. If Y, set_params and set_attributes have been given for parameters.
        or
        `Dataset`
            A Dataset object.
        """
        if Y.size > 0 and len(self.get_scale_Y_props) > 0:
            return Dataset.unscale_Y(
                Y,
                method,
                self.get_scale_Y_props["params"],
                self.get_scale_Y_props["attributes"],
            )
        elif len(self.get_scale_Y_props) > 0:
            self.Y = Dataset.unscale_Y(
                self.Y,
                method,
                self.get_scale_Y_props["params"],
                self.get_scale_Y_props["attributes"],
            )
        else:
            self.Y = Dataset.unscale_Y(self.Y, method, set_params, set_attributes)
        return self

    def unscale_Y(
        Y: np.ndarray, method: Scale, set_params: List = [], set_attributes: List = []
    ):
        """
        Static unscale method for a 1D vector or 2D matrix of the output property(ies) with Scale method.

        Parameters
        ----------

        Y: `numpy.ndarray`
                A scaled 1D vector or 2D matrix of the output property(ies)

        method: `Scale`
                The method is used to scale Y

        set_params: `List`
                A list of dicts with the parameters of each Scale method.

        set_attributes: `List`
                A list of dicts with the attributes of each Scale method.


        Returns
        -------
        `numpy.ndarray`
            The original 1D or 2D matrix Y
        """
        if len(set_attributes) == 0:
            raise AssertionError("You need to specify set_attributes")
        if method == Scale.STANDARD:
            scaler = [StandardScaler() for _ in range(Y.shape[1])]
        elif method == Scale.MINMAX:
            scaler = [MinMaxScaler() for _ in range(Y.shape[1])]
        Y = Dataset.__unscale_Y_parser(Y, method, scaler, set_params, set_attributes)
        return Y

    def __unscale_Y_parser(
        Y: np.ndarray,
        method: Scale,
        scaler: Any,
        set_params: Dict,
        set_attributes: Dict,
    ):
        for i in range(Y.shape[1]):
            if len(set_params) != 0:
                scaler[i] = scaler[i].set_params(**set_params[i])
            scaler[i] = Dataset.__set_scale_attributes(
                method, scaler[i], set_attributes[i]
            )
            Y[:, i] = scaler[i].inverse_transform(Y[:, i].reshape(-1, 1)).flatten()
        return Y

    def apply_scale_Y(
        self, method: Scale, set_params: List = [], set_attributes: List = []
    ):
        """
        Scale a 1D vector or 2D matrix of the output property(ies) with Scale method.

        Parameters
        ----------

        method: `Scale`
                The method is used to scale Y

        set_params: `List`
                A list of dicts with the parameters of each Scale method.

        set_attributes: `List`
                A list of dicts with the attributes of each Scale method.


        Returns
        -------
        `Dataset`
             A Dataset object.
        """
        self.Y, self.get_scale_Y_props = Dataset.scale_Y(
            self.Y, method, set_params, set_attributes
        )
        return self

    def scale_Y(
        Y: np.ndarray, method: Scale, set_params: List = [], set_attributes: List = []
    ):
        """
        Static scale method for a 1D vector or 2D matrix of the output property(ies) with Scale method.

        Parameters
        ----------

        Y: `numpy.ndarray`
                A scaled 1D vector or 2D matrix of the output property(ies).

        method: `Scale`
                The method is used to scale Y

        set_params: `List`
                A list of dicts with the parameters of each Scale method.

        set_attributes: `List`
                A list of dicts with the attributes of each Scale method.


        Returns
        -------
        `Dataset`
             A Dataset object.
        """
        if method == Scale.STANDARD:
            scaler = [StandardScaler() for _ in range(Y.shape[1])]
        elif method == Scale.MINMAX:
            scaler = [MinMaxScaler() for _ in range(Y.shape[1])]
        Y, get_params, get_attributes = Dataset.__scale_Y_parser(
            Y, method, scaler, set_params, set_attributes
        )
        return Y, {"params": get_params, "attributes": get_attributes}

    def __scale_Y_parser(
        Y: np.ndarray,
        method: Scale,
        scaler: Any,
        set_params: Dict,
        set_attributes: Dict,
    ):
        get_params = []
        get_attributes = []
        for i in range(Y.shape[1]):
            if len(set_params) != 0:
                scaler[i] = scaler[i].set_params(**set_params[i])
            if len(set_attributes) != 0:
                scaler[i] = Dataset.__set_scale_attributes(
                    method, scaler[i], set_attributes[i]
                )
            else:
                scaler[i] = scaler[i].fit(Y[:, i].reshape(-1, 1))
            Y[:, i] = scaler[i].transform(Y[:, i].reshape(-1, 1)).flatten()
            get_params.append(scaler[i].get_params())
            get_attributes.append(Dataset.__get_scale_attributes(method, scaler[i]))
        return Y, get_params, get_attributes

    def __preprocess(self, X: np.ndarray, method: SpectralPreprocessingSequence):
        if isinstance(method, str):
            return Spectra(X).apply(method).X
        elif isinstance(method, tuple):
            return Spectra(X).apply(method[0], **method[1]).X
        elif isinstance(method, list):
            for each_method in method:
                if isinstance(each_method, str):
                    X = Spectra(X).apply(each_method).X
                elif isinstance(each_method, tuple):
                    X = Spectra(X).apply(each_method[0], **each_method[1]).X
                elif isinstance(each_method, list):
                    X = self.__preprocess(X, each_method)
            return X

    def __set_scale_attributes(method: Scale, scaler: Any, set_attributes: Dict):
        if method == Scale.STANDARD:
            scaler.scale_ = set_attributes["scale_"]
            scaler.mean_ = set_attributes["mean_"]
            scaler.var_ = set_attributes["var_"]
            scaler.n_samples_seen_ = set_attributes["n_samples_seen_"]
        elif method == Scale.MINMAX:
            scaler.min_ = set_attributes["min_"]
            scaler.scale_ = set_attributes["scale_"]
            scaler.data_min_ = set_attributes["data_min_"]
            scaler.data_max_ = set_attributes["data_max_"]
            scaler.data_range_ = set_attributes["data_range_"]
        return scaler

    def __get_scale_attributes(method: Scale, scaler: Any):
        if method == Scale.STANDARD:
            return {
                "scale_": scaler.scale_,
                "mean_": scaler.mean_,
                "var_": scaler.var_,
                "n_samples_seen_": scaler.n_samples_seen_,
            }
        elif method == Scale.MINMAX:
            return {
                "min_": scaler.min_,
                "scale_": scaler.scale_,
                "data_min_": scaler.data_min_,
                "data_max_": scaler.data_max_,
                "data_range_": scaler.data_range_,
            }

    def __scale_X(
        X: np.ndarray,
        method: Scale,
        scaler: Any,
        set_params: Dict,
        set_attributes: Dict,
    ):
        if len(set_params) != 0:
            scaler = scaler.set_params(**set_params)
        get_params = scaler.get_params()
        if method == Scale.STANDARD:
            if len(set_attributes) != 0:
                scaler = Dataset.__set_scale_attributes(method, scaler, set_attributes)
            else:
                scaler = scaler.fit(X)
            X = scaler.transform(X)
            get_attributes = Dataset.__get_scale_attributes(method, scaler)
        elif method == Scale.MINMAX:
            if len(set_attributes) != 0:
                scaler = Dataset.__set_scale_attributes(method, scaler, set_attributes)
            else:
                scaler = scaler.fit(X)
            X = scaler.transform(X)
            get_attributes = Dataset.__get_scale_attributes(method, scaler)
        return X, get_params, get_attributes
